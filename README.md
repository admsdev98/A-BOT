#  A-BOT - Asistente Virtual de Adam

## Descripci贸n

A-BOT es un asistente virtual dise帽ado por y para el usuario, orientado a responder consultas sobre la informaci贸n que este defina. 

Se caracteriza por una configuraci贸n sencilla, uso intuitivo y rendimiento 谩gil.

## Caracter铆sticas

- **Chat interactivo**, porque leer ficheros a veces es aburrido.
- **Informaci贸n personal**, permite responder en base a la informaci贸n definida por el usuario.
- **IA como agente**, compatible con modelos locales a trav茅s de Ollama o integrable mediante API con cualquier otro modelo.
- **Simple y r谩pido**, backend en FastAPI y frontend en Streamlit.
- **Dockerizado**, despliegue f谩cil y sencillo para cualquier desarrollador en su equipo.

## Tecnolog铆as Utilizadas

### Backend
Python / FastAPI / Uvicorn / LangChain / Ollama / Pydantic

### Frontend
Streamlit / HTTPX (cliente HTTP as铆ncrono)

### IA
Ollama / GPT-3.5 

### Otros
Docker / Docker Compose

## Importante

> **Primera ejecuci贸n lenta**: Si utilizas el modelo local configurado por defecto (`phi4-mini`) u otro compatible, la primera vez que ejecutes el proyecto tardar谩 varios minutos en descargar el modelo de IA (~3GB).

## Instalaci贸n y Configuraci贸n


### Prerrequisitos

- **Docker** y **Docker Compose** instalados
- **4GB+ de RAM** para ejecutar el modelo LLM preconfigurado

   **Obligatorio**: A-BOT requiere Ollama instalado en tu sistema para funcionar con modelos locales.

   1. **Instalar Ollama:**
   [Descargar desde la p谩gina oficial][ollama-download]

   [ollama-download]: https://ollama.ai/download

   2. **Descargar el modelo requerido:**
      ```bash
      ollama pull phi4-mini
      ```
   [ollama-models]: https://github.com/ollama/ollama?tab=readme-ov-file#model-library

   3. **Verificar la instalaci贸n:**
      ```bash
      ollama list
      ```


### Instalaci贸n con Docker (Recomendado)

1. **Clonar el repositorio:**
   ```bash
   git clone <url-del-repositorio>
   cd A-BOT
   ```

2. **Ejecutar con Docker Compose:**
   ```bash
   cd docker
   docker-compose up --build
   ```

3. **Acceder a la aplicaci贸n:**
   - Frontend: http://localhost:8501
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

### Acceso a contenedores

```bash
# Frontend (Streamlit)
docker-compose exec frontend streamlit run app.py --server.port 8501

# Backend (FastAPI) - Modelo local
docker-compose exec backend uvicorn main:app --reload --port 8000

# Backend (FastAPI) - Modelo OpenAI
docker-compose exec backend bash -c "ABOT_MODEL=openai uvicorn main:app --reload --port 8000"