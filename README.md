# A-BOT: Chatbot Local con Llama

Un chatbot inteligente y conversacional basado en modelos Llama que funciona completamente de forma local, sin necesidad de conexiÃ³n a internet para el procesamiento de lenguaje natural.

## ğŸš€ CaracterÃ­sticas

- **Procesamiento 100% local**: No requiere conexiÃ³n a internet para funcionar
- **Modelos Llama**: Utiliza modelos Llama 2/3 para respuestas inteligentes
- **Interfaz conversacional**: Chat interactivo y fÃ¡cil de usar
- **ConfiguraciÃ³n flexible**: Soporte para diferentes tamaÃ±os de modelo
- **Memoria de conversaciÃ³n**: Mantiene contexto de la conversaciÃ³n
- **MÃºltiples formatos**: Soporte para texto, cÃ³digo y respuestas estructuradas

## ğŸ“‹ Requisitos Previos

- **Python 3.8+**
- **Git**
- **Al menos 8GB de RAM** (recomendado 16GB+)
- **GPU compatible con CUDA** (opcional, para aceleraciÃ³n)

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <tu-repositorio>
cd A-BOT
```

### 2. Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # En Linux/Mac
# o
venv\Scripts\activate     # En Windows
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### 4. Descargar modelo Llama
```bash
# OpciÃ³n 1: Usando Hugging Face Hub
python scripts/download_model.py --model "meta-llama/Llama-2-7b-chat-hf"

# OpciÃ³n 2: Descarga manual desde Hugging Face
# Visita: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
```

## ğŸ¯ Uso RÃ¡pido

### Iniciar el chatbot
```bash
python main.py
```

### Uso desde Python
```python
from chatbot import LlamaChatbot

# Inicializar chatbot
bot = LlamaChatbot(model_path="models/llama-2-7b-chat")

# Hacer una pregunta
response = bot.chat("Â¿CuÃ¡l es la capital de EspaÃ±a?")
print(response)
```

### Interfaz web (opcional)
```bash
python web_interface.py
# Abrir http://localhost:8000 en tu navegador
```

## ğŸ“ Estructura del Proyecto

```
A-BOT/
â”œâ”€â”€ README.md                 # Este archivo
â”œâ”€â”€ requirements.txt          # Dependencias de Python
â”œâ”€â”€ main.py                  # Punto de entrada principal
â”œâ”€â”€ chatbot.py               # Clase principal del chatbot
â”œâ”€â”€ models/                  # Directorio para modelos descargados
â”œâ”€â”€ config/                  # Archivos de configuraciÃ³n
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ chat_config.yaml
â”œâ”€â”€ scripts/                 # Scripts de utilidad
â”‚   â”œâ”€â”€ download_model.py
â”‚   â””â”€â”€ benchmark.py
â”œâ”€â”€ web/                     # Interfaz web (opcional)
â”‚   â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ static/
â”‚   â””â”€â”€ web_interface.py
â”œâ”€â”€ tests/                   # Tests unitarios
â””â”€â”€ examples/                # Ejemplos de uso
```

## âš™ï¸ ConfiguraciÃ³n

### ConfiguraciÃ³n del modelo
Edita `config/model_config.yaml`:

```yaml
model:
  name: "llama-2-7b-chat"
  path: "models/llama-2-7b-chat"
  max_length: 2048
  temperature: 0.7
  top_p: 0.9
  device: "auto"  # "cpu", "cuda", o "auto"
```

### ConfiguraciÃ³n del chat
Edita `config/chat_config.yaml`:

```yaml
chat:
  max_history: 10
  system_prompt: "Eres un asistente Ãºtil y amigable."
  enable_memory: true
  save_conversations: true
```

## ğŸ”§ Opciones Avanzadas

### Usar GPU para aceleraciÃ³n
```bash
# Instalar PyTorch con soporte CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Cambiar modelo
```bash
# Descargar modelo diferente
python scripts/download_model.py --model "meta-llama/Llama-2-13b-chat-hf"

# O usar modelo local
python main.py --model-path /ruta/a/tu/modelo
```

### Benchmark de rendimiento
```bash
python scripts/benchmark.py --model llama-2-7b-chat
```

## ğŸ“Š Modelos Soportados

| Modelo | TamaÃ±o | RAM MÃ­nima | Calidad |
|--------|--------|------------|---------|
| Llama-2-7b-chat | 7B | 8GB | Buena |
| Llama-2-13b-chat | 13B | 16GB | Mejor |
| Llama-2-70b-chat | 70B | 64GB | Excelente |
| Llama-3-8b-instruct | 8B | 12GB | Muy buena |

## ğŸ› SoluciÃ³n de Problemas

### Error: "CUDA out of memory"
- Reduce el tamaÃ±o del modelo
- Usa `device: "cpu"` en la configuraciÃ³n
- Cierra otras aplicaciones que usen GPU

### Error: "Model not found"
- Verifica que el modelo estÃ© descargado en `models/`
- Ejecuta `python scripts/download_model.py`

### Rendimiento lento
- Usa GPU si estÃ¡ disponible
- Reduce `max_length` en la configuraciÃ³n
- Usa un modelo mÃ¡s pequeÃ±o

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- [Meta AI](https://ai.meta.com/) por los modelos Llama
- [Hugging Face](https://huggingface.co/) por la infraestructura de modelos
- [Transformers](https://github.com/huggingface/transformers) por la biblioteca de procesamiento

## ğŸ“ Soporte

Si tienes problemas o preguntas:

- Abre un [Issue](https://github.com/tu-usuario/A-BOT/issues)
- Consulta la [documentaciÃ³n](docs/)
- Ãšnete a nuestro [Discord](https://discord.gg/tu-servidor)

---

**Â¡Disfruta conversando con tu chatbot local! ğŸ¤–âœ¨** 