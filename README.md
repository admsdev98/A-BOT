#  A-BOT - Asistente Virtual de Adam

## Descripci贸n

A-BOT es un asistente virtual dise帽ado por y para el usuario, orientado a responder consultas sobre la informaci贸n que este defina. 

Se caracteriza por una configuraci贸n sencilla, uso intuitivo y rendimiento 谩gil.

## Caracter铆sticas

- **Chat interactivo**, porque leer ficheros a veces es aburrido.
- **Informaci贸n personal**, permite responder en base a la informaci贸n definida por el usuario.
- **IA como agente**, compatible con modelos locales a trav茅s de Ollama o integrable mediante API con cualquier otro modelo.
- **Simple y r谩pido**, backend en FastAPI y frontend en Streamlit.
- **Dockerizado**, despliegue f谩cil y sencillo para cualquier desarrollador en su equipo.

## Tecnolog铆as Utilizadas

### Backend
Python / FastAPI / Uvicorn / LangChain / Ollama / Pydantic

### Frontend
Streamlit / HTTPX (cliente HTTP as铆ncrono)

### IA
Ollama / GPT-3.5 

### Otros
Docker / Docker Compose

## Importante

> **Primera ejecuci贸n lenta**: Si utilizas el modelo local configurado por defecto (`phi4-mini`) u otro compatible, la primera vez que ejecutes el proyecto tardar谩 varios minutos en descargar el modelo de IA (~3GB).

## Instalaci贸n y Configuraci贸n

### Prerrequisitos

- **Docker** y **Docker Compose** instalados
- **4GB+ de RAM** para ejecutar el modelo LLM preconfigurado


### Instalaci贸n con Docker (Recomendado)

1. **Clonar el repositorio:**
   ```bash
   git clone <url-del-repositorio>
   cd A-BOT
   ```

2. **Ejecutar con Docker Compose:**
   ```bash
   cd docker
   docker-compose up --build
   ```

3. **Acceder a la aplicaci贸n:**
   - Frontend: http://localhost:8501
   - Backend API: http://localhost:8000

### Instalaci贸n Local (sin Docker)

1. **Clonar el repositorio:**
   ```bash
   git clone <url-del-repositorio>
   cd A-BOT
   ```

2. **Crear entorno virtual:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # o
   venv\Scripts\activate     # Windows
   ```

3. **Instalar dependencias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Instalar Ollama:**
   ```bash
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

5. **Descargar el modelo predefinido:**
   ```bash
   ollama pull phi4-mini
   ```

6. **Ejecutar el backend:**
   ```bash
   cd src/backend
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

7. **Ejecutar el frontend (nueva terminal):**
   ```bash
   cd src/frontend
   streamlit run app.py
   ```